# -*- coding: utf-8 -*-
"""GBC FINAL MODEL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F__nFQ1uJYXmccJ7JSkLQ1-x-5EcZoWj
"""

import pandas as pd
import numpy as np
from sklearn.cross_validation import train_test_split

from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier


train = "https://raw.githubusercontent.com/12ani/Taarifa_water_table/master/pump_train_for_models%20(1).csv" 
test =  "https://raw.githubusercontent.com/12ani/Taarifa_water_table/master/water_table_test.csv"
train = pd.read_csv(train)
test = pd.read_csv(test)

dummy_cols = ['funder', 'installer', 'basin', 'public_meeting', 'scheme_management', 'permit',
              'construction_year', 'management_group', 'payment', 'water_quality',
              'quantity', 'source_class', 'waterpoint_type']

train = pd.get_dummies(train, columns = dummy_cols)

train = train.sample(frac=1).reset_index(drop=True)

test = pd.get_dummies(test, columns = dummy_cols)
test

# Let's split the train set into train and validation sets. Also remove the target.

target = train.status_group
features = train.drop('status_group', axis=1)

X_train, X_val, y_train, y_val = train_test_split(features, target, train_size=0.8)

gbc = GradientBoostingClassifier(n_estimators=100,learning_rate=0.075,max_features=1.0,min_samples_leaf=16,max_depth=14)
gbc
#{'min_samples_leaf': 16, 'n_estimators': 100, 'learning_rate': 0.075, 'max_features': 1.0, 'max_depth': 14}
#Validation accuracy:  0.796043771044

gbc_model = gbc.fit(X_train,y_train)

val_acc = gbc_model.score(X_val,y_val)
val_acc

scores = gbc_model.score(X_train,y_train)
scores

from sklearn.externals import joblib
filename = 'gbc_model.sav'

joblib.dump(gbc_model,filename)

loaded_model = joblib.load(filename)

y_test=loaded_model.score(X_val,y_val)

y_test

features

pred = loaded_model.predict(X_train)
pred

ytrain=y_train.values
ytrain

from sklearn import metrics

metrics.confusion_matrix(ytrain,pred)

from sklearn.metrics import classification_report

from sklearn.metrics import classification_report
#target_names = ['functional', 'functional needs repair', 'non functional']
print(classification_report(ytrain, pred))



final_gbc=loaded_model.fit(features, target)     
predictions = loaded_model.predict(test)

final_filename = 'final_gbc.sav'
joblib.dump(final_gbc,final_filename)

final_loaded = joblib.load(final_filename)
final_loaded

predictions

importances = gbc_model.feature_importances_
importances

indices = np.argsort(importances)[::-1]

# Print the feature ranking
print("Feature ranking:")

for f in range(X_val.shape[1]):
    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))
    print(X_val.columns[indices[f]],end=',')
    print()

predictions=pd.DataFrame(predictions)
predictions

pred_id = pd.read_csv('https://raw.githubusercontent.com/12ani/Taarifa_water_table/master/test_set_values.csv', usecols=['id'])
pred_id

final=pd.concat([pred_id,predictions],axis=1)
final

final.columns=['id','status_group']
final.to_csv('predictions_GBC.csv', index=False)